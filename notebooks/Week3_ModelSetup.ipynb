{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c76d533",
   "metadata": {},
   "source": [
    "Import Libraries & Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1c1c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: seaborn in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: nibabel in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (5.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torchvision) (2.3.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from nibabel) (25.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arnav\\desktop\\medbind3d\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 seaborn nibabel tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efda50e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WEEK-3: SWIN TRANSFORMER 3D - MODEL SETUP\n",
      "======================================================================\n",
      "PyTorch Version: 2.7.1+cu118\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.00 GB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"WEEK-3: SWIN TRANSFORMER 3D - MODEL SETUP\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c272a",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0787b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Data directory: ..\\processed_data\n",
      "  Patch size: (64, 64, 64)\n",
      "  Input channels: 4\n",
      "  Number of classes: 4\n",
      "  Batch size: 1\n",
      "  Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_DIR = os.path.join('..', 'processed_data')\n",
    "    \n",
    "    # Data parameters\n",
    "    NUM_CLASSES = 4  # Background (0), Necrotic (1), Edema (2), Enhancing (4)\n",
    "    IN_CHANNELS = 4  # FLAIR, T1, T1CE, T2\n",
    "    PATCH_SIZE = (64, 64, 64)  # Adjust based on your preprocessed patches\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 1  # Start small for sanity check (increase to 2 if memory allows)\n",
    "    NUM_WORKERS = 0  # 0 for Windows, increase for Linux\n",
    "    \n",
    "    # Model parameters\n",
    "    EMBED_DIM = 48  # Base embedding dimension\n",
    "    DEPTHS = [2, 2, 2, 2]  # Number of Swin blocks in each stage\n",
    "    NUM_HEADS = [3, 6, 12, 24]  # Number of attention heads per stage\n",
    "    WINDOW_SIZE = (4, 4, 4)  # Window size for local attention\n",
    "    \n",
    "    # Optimization parameters\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    MAX_EPOCHS = 100  # For future full training\n",
    "    \n",
    "    # Loss weights\n",
    "    DICE_WEIGHT = 0.5\n",
    "    CE_WEIGHT = 0.5\n",
    "    \n",
    "    # Sanity check\n",
    "    SANITY_CHECK_BATCHES = 2\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Data directory: {config.DATA_DIR}\")\n",
    "print(f\"  Patch size: {config.PATCH_SIZE}\")\n",
    "print(f\"  Input channels: {config.IN_CHANNELS}\")\n",
    "print(f\"  Number of classes: {config.NUM_CLASSES}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61742128",
   "metadata": {},
   "source": [
    "BRATS DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9f3b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING DATASETS\n",
      "======================================================================\n",
      "✓ TRAIN Dataset initialized: 21 samples\n",
      "✓ VAL Dataset initialized: 5 samples\n",
      "\n",
      "Dataset Statistics:\n",
      "  Training samples: 21\n",
      "  Validation samples: 5\n",
      "\n",
      "Sample Shapes:\n",
      "  Image: torch.Size([4, 64, 64, 64]) (4 modalities × D × H × W)\n",
      "  Mask: torch.Size([64, 64, 64]) (D × H × W)\n",
      "  Image dtype: torch.float32\n",
      "  Mask dtype: torch.int64\n",
      "  Mask unique values: tensor([0, 1, 2, 3])\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for BraTS 3D brain tumor segmentation.\n",
    "    \n",
    "    Loads preprocessed .npy patches (images + masks) from disk.\n",
    "    \n",
    "    Returns:\n",
    "        image: Tensor of shape (4, D, H, W) - 4 MRI modalities\n",
    "        mask: Tensor of shape (D, H, W) - segmentation labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Root directory containing train/val folders\n",
    "            split: 'train' or 'val'\n",
    "            transform: Optional transforms to apply\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Paths to images and masks\n",
    "        self.image_dir = os.path.join(data_dir, split, 'images')\n",
    "        self.mask_dir = os.path.join(data_dir, split, 'masks')\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith('.npy')])\n",
    "        self.mask_files = sorted([f for f in os.listdir(self.mask_dir) if f.endswith('.npy')])\n",
    "        \n",
    "        assert len(self.image_files) == len(self.mask_files), \\\n",
    "            f\"Mismatch: {len(self.image_files)} images vs {len(self.mask_files)} masks\"\n",
    "        \n",
    "        print(f\"✓ {split.upper()} Dataset initialized: {len(self.image_files)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        \n",
    "        image = np.load(image_path)  # Shape: (4, D, H, W)\n",
    "        mask = np.load(mask_path)    # Shape: (D, H, W)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        image = torch.from_numpy(image).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        # Remap labels: BraTS uses {0, 1, 2, 4} → Map to {0, 1, 2, 3}\n",
    "        # This is necessary for CrossEntropyLoss which expects contiguous labels\n",
    "        mask[mask == 4] = 3\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Test dataset loading\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_dataset = BraTSDataset(config.DATA_DIR, split='train')\n",
    "val_dataset = BraTSDataset(config.DATA_DIR, split='val')\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Test loading one sample\n",
    "sample_image, sample_mask = train_dataset[0]\n",
    "print(f\"\\nSample Shapes:\")\n",
    "print(f\"  Image: {sample_image.shape} (4 modalities × D × H × W)\")\n",
    "print(f\"  Mask: {sample_mask.shape} (D × H × W)\")\n",
    "print(f\"  Image dtype: {sample_image.dtype}\")\n",
    "print(f\"  Mask dtype: {sample_mask.dtype}\")\n",
    "print(f\"  Mask unique values: {torch.unique(sample_mask)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35db652",
   "metadata": {},
   "source": [
    "Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0377422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoaders created:\n",
      "  Train batches: 21\n",
      "  Val batches: 5\n",
      "  Batch size: 1\n",
      "\n",
      "Testing DataLoader...\n",
      "  Batch 1:\n",
      "    Images shape: torch.Size([1, 4, 64, 64, 64])\n",
      "    Masks shape: torch.Size([1, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(\"✓ DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "\n",
    "# Test dataloader\n",
    "print(\"\\nTesting DataLoader...\")\n",
    "for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "    print(f\"  Batch {batch_idx+1}:\")\n",
    "    print(f\"    Images shape: {images.shape}\")\n",
    "    print(f\"    Masks shape: {masks.shape}\")\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c240df",
   "metadata": {},
   "source": [
    "Swin Transformer 3D Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5e549b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Swin Transformer 3D building blocks defined\n",
      "  • PatchEmbed3D\n",
      "  • WindowAttention3D\n",
      "  • SwinTransformerBlock3D\n",
      "  • window_partition (with automatic padding)\n",
      "  • window_reverse\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbed3D(nn.Module):\n",
    "    \"\"\"\n",
    "    3D Patch Embedding layer.\n",
    "    Splits input volume into non-overlapping patches and projects to embedding dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=4, in_chans=4, embed_dim=96):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Use 3D convolution for patch projection\n",
    "        self.proj = nn.Conv3d(\n",
    "            in_chans, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, D, H, W)\n",
    "        x = self.proj(x)  # (B, embed_dim, D/p, H/p, W/p)\n",
    "        \n",
    "        # Reshape for transformer: (B, D/p, H/p, W/p, embed_dim)\n",
    "        B, C, D, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, D*H*W, embed_dim)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Reshape back to 3D grid\n",
    "        x = x.transpose(1, 2).view(B, C, D, H, W)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Partition input into windows with padding if necessary.\n",
    "    \n",
    "    Args:\n",
    "        x: (B, D, H, W, C)\n",
    "        window_size: (Wd, Wh, Ww)\n",
    "    \n",
    "    Returns:\n",
    "        windows: (B*num_windows, Wd*Wh*Ww, C)\n",
    "    \"\"\"\n",
    "    B, D, H, W, C = x.shape\n",
    "    Wd, Wh, Ww = window_size\n",
    "    \n",
    "    # Pad if necessary\n",
    "    pad_d = (Wd - D % Wd) % Wd\n",
    "    pad_h = (Wh - H % Wh) % Wh\n",
    "    pad_w = (Ww - W % Ww) % Ww\n",
    "    \n",
    "    if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h, 0, pad_d))\n",
    "    \n",
    "    B, D, H, W, C = x.shape\n",
    "    \n",
    "    # Partition into windows\n",
    "    x = x.view(B, D // Wd, Wd, H // Wh, Wh, W // Ww, Ww, C)\n",
    "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous()\n",
    "    windows = windows.view(-1, Wd * Wh * Ww, C)\n",
    "    \n",
    "    return windows, (D, H, W)\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, original_size):\n",
    "    \"\"\"\n",
    "    Reverse window partition.\n",
    "    \n",
    "    Args:\n",
    "        windows: (B*num_windows, Wd*Wh*Ww, C)\n",
    "        window_size: (Wd, Wh, Ww)\n",
    "        original_size: (D, H, W) - padded size\n",
    "    \n",
    "    Returns:\n",
    "        x: (B, D, H, W, C)\n",
    "    \"\"\"\n",
    "    Wd, Wh, Ww = window_size\n",
    "    D, H, W = original_size\n",
    "    C = windows.shape[-1]\n",
    "    \n",
    "    B = int(windows.shape[0] / (D * H * W / Wd / Wh / Ww))\n",
    "    \n",
    "    x = windows.view(B, D // Wd, H // Wh, W // Ww, Wd, Wh, Ww, C)\n",
    "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous()\n",
    "    x = x.view(B, D, H, W, C)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based multi-head self-attention for 3D data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # (Wd, Wh, Ww)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        \n",
    "        # Q, K, V projections\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B*num_windows, window_size^3, dim)\n",
    "        B_, N, C = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B_, num_heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer Block with Window Attention + MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, window_size, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention3D(dim, window_size, num_heads)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_hidden_dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, D, H, W, C)\n",
    "        B, D, H, W, C = x.shape\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Window partition with padding\n",
    "        x_windows, padded_size = window_partition(x, self.window_size)\n",
    "        \n",
    "        # Window attention\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        \n",
    "        # Reverse window partition\n",
    "        x = window_reverse(attn_windows, self.window_size, padded_size)\n",
    "        \n",
    "        # Crop back to original size if padding was added\n",
    "        x = x[:, :D, :H, :W, :].contiguous()\n",
    "        \n",
    "        # Skip connection\n",
    "        x = shortcut + x\n",
    "        \n",
    "        # MLP\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✓ Swin Transformer 3D building blocks defined\")\n",
    "print(\"  • PatchEmbed3D\")\n",
    "print(\"  • WindowAttention3D\")\n",
    "print(\"  • SwinTransformerBlock3D\")\n",
    "print(\"  • window_partition (with automatic padding)\")\n",
    "print(\"  • window_reverse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec2dce",
   "metadata": {},
   "source": [
    "Swin Transformer 3D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb34ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INITIALIZING MODEL\n",
      "======================================================================\n",
      "✓ Model initialized\n",
      "\n",
      "Model Statistics:\n",
      "  Total parameters: 6,429,076\n",
      "  Trainable parameters: 6,429,076\n",
      "  Model size: ~24.52 MB (float32)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class SwinTransformer3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer 3D for medical image segmentation.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Patch Embedding\n",
    "        2. Multiple Swin Transformer stages\n",
    "        3. Upsampling decoder\n",
    "        4. Final segmentation head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans=4,\n",
    "        num_classes=4,\n",
    "        embed_dim=48,\n",
    "        depths=[2, 2, 2, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=(4, 4, 4),\n",
    "        patch_size=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbed3D(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        # Swin Transformer stages\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = nn.ModuleList([\n",
    "                SwinTransformerBlock3D(\n",
    "                    dim=int(embed_dim * 2 ** i_layer),\n",
    "                    num_heads=num_heads[i_layer],\n",
    "                    window_size=window_size\n",
    "                )\n",
    "                for _ in range(depths[i_layer])\n",
    "            ])\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        # Patch merging (downsampling) between stages\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers - 1):\n",
    "            downsample_layer = nn.Conv3d(\n",
    "                int(embed_dim * 2 ** i_layer),\n",
    "                int(embed_dim * 2 ** (i_layer + 1)),\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "        \n",
    "        # Decoder (upsampling path)\n",
    "        self.upsample_layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers - 1, 0, -1):\n",
    "            upsample_layer = nn.Sequential(\n",
    "                nn.ConvTranspose3d(\n",
    "                    int(embed_dim * 2 ** i_layer),\n",
    "                    int(embed_dim * 2 ** (i_layer - 1)),\n",
    "                    kernel_size=2,\n",
    "                    stride=2\n",
    "                ),\n",
    "                nn.BatchNorm3d(int(embed_dim * 2 ** (i_layer - 1))),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.upsample_layers.append(upsample_layer)\n",
    "        \n",
    "        # Final upsampling to original resolution\n",
    "        self.final_upsample = nn.Sequential(\n",
    "            nn.ConvTranspose3d(embed_dim, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.BatchNorm3d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Segmentation head\n",
    "        self.segmentation_head = nn.Conv3d(embed_dim, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, D, H, W)\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, embed_dim, D/p, H/p, W/p)\n",
    "        \n",
    "        # Store features for skip connections (optional)\n",
    "        encoder_features = []\n",
    "        \n",
    "        # Encoder path\n",
    "        for i, layer_blocks in enumerate(self.layers):\n",
    "            # Prepare for transformer blocks (need channel-last format)\n",
    "            B, C, D, H, W = x.shape\n",
    "            x = x.permute(0, 2, 3, 4, 1).contiguous()  # (B, D, H, W, C)\n",
    "            \n",
    "            # Apply Swin blocks\n",
    "            for block in layer_blocks:\n",
    "                x = block(x)\n",
    "            \n",
    "            # Back to channel-first\n",
    "            x = x.permute(0, 4, 1, 2, 3).contiguous()  # (B, C, D, H, W)\n",
    "            \n",
    "            encoder_features.append(x)\n",
    "            \n",
    "            # Downsample (except last layer)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = self.downsample_layers[i](x)\n",
    "        \n",
    "        # Decoder path (simple upsampling without skip connections for now)\n",
    "        for i, upsample_layer in enumerate(self.upsample_layers):\n",
    "            x = upsample_layer(x)\n",
    "        \n",
    "        # Final upsampling to original size\n",
    "        x = self.final_upsample(x)\n",
    "        \n",
    "        # Segmentation head\n",
    "        x = self.segmentation_head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIALIZING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = SwinTransformer3D(\n",
    "    in_chans=config.IN_CHANNELS,\n",
    "    num_classes=config.NUM_CLASSES,\n",
    "    embed_dim=config.EMBED_DIM,\n",
    "    depths=config.DEPTHS,\n",
    "    num_heads=config.NUM_HEADS,\n",
    "    window_size=config.WINDOW_SIZE,\n",
    "    patch_size=4\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Model initialized\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024**2:.2f} MB (float32)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e4f94",
   "metadata": {},
   "source": [
    "Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5163137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loss functions initialized:\n",
      "  Combined Loss = 0.5 × Dice + 0.5 × CrossEntropy\n"
     ]
    }
   ],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for multi-class segmentation.\n",
    "    \n",
    "    Dice = 2 * |X ∩ Y| / (|X| + |Y|)\n",
    "    Loss = 1 - Dice\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, predictions, targets, num_classes=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (B, C, D, H, W) - raw logits\n",
    "            targets: (B, D, H, W) - class indices\n",
    "        \"\"\"\n",
    "        # Convert predictions to probabilities\n",
    "        predictions = F.softmax(predictions, dim=1)\n",
    "        \n",
    "        # One-hot encode targets\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=num_classes)  # (B, D, H, W, C)\n",
    "        targets_one_hot = targets_one_hot.permute(0, 4, 1, 2, 3).float()  # (B, C, D, H, W)\n",
    "        \n",
    "        # Calculate Dice score for each class\n",
    "        dice_scores = []\n",
    "        for class_idx in range(num_classes):\n",
    "            pred_class = predictions[:, class_idx]\n",
    "            target_class = targets_one_hot[:, class_idx]\n",
    "            \n",
    "            intersection = (pred_class * target_class).sum()\n",
    "            union = pred_class.sum() + target_class.sum()\n",
    "            \n",
    "            dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "            dice_scores.append(dice)\n",
    "        \n",
    "        # Average Dice across classes\n",
    "        dice_score = torch.stack(dice_scores).mean()\n",
    "        \n",
    "        return 1 - dice_score\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Dice + Cross-Entropy Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, dice_weight=0.5, ce_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        \n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (B, C, D, H, W) - raw logits\n",
    "            targets: (B, D, H, W) - class indices\n",
    "        \"\"\"\n",
    "        dice = self.dice_loss(predictions, targets)\n",
    "        ce = self.ce_loss(predictions, targets)\n",
    "        \n",
    "        combined = self.dice_weight * dice + self.ce_weight * ce\n",
    "        \n",
    "        return combined, dice, ce\n",
    "\n",
    "\n",
    "# Initialize loss\n",
    "criterion = CombinedLoss(\n",
    "    dice_weight=config.DICE_WEIGHT,\n",
    "    ce_weight=config.CE_WEIGHT\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Loss functions initialized:\")\n",
    "print(f\"  Combined Loss = {config.DICE_WEIGHT} × Dice + {config.CE_WEIGHT} × CrossEntropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dfcbc",
   "metadata": {},
   "source": [
    "OPTIMIZER & SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b83989c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Optimizer & Scheduler initialized:\n",
      "  Optimizer: AdamW\n",
      "  Learning rate: 0.0001\n",
      "  Weight decay: 1e-05\n",
      "  Scheduler: CosineAnnealingLR\n",
      "  T_max: 100 epochs\n"
     ]
    }
   ],
   "source": [
    "# AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.MAX_EPOCHS,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizer & Scheduler initialized:\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {config.WEIGHT_DECAY}\")\n",
    "print(f\"  Scheduler: CosineAnnealingLR\")\n",
    "print(f\"  T_max: {config.MAX_EPOCHS} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa631d",
   "metadata": {},
   "source": [
    "TRAINING & VALIDATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80cc0635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training & validation functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_ce = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    \n",
    "    for batch_idx, (images, masks) in enumerate(progress_bar):\n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss, dice_loss, ce_loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        total_dice += dice_loss.item()\n",
    "        total_ce += ce_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'dice': f'{dice_loss.item():.4f}',\n",
    "            'ce': f'{ce_loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_dice = total_dice / len(dataloader)\n",
    "    avg_ce = total_ce / len(dataloader)\n",
    "    \n",
    "    return avg_loss, avg_dice, avg_ce\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \n",
    "    Returns:\n",
    "        Average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_ce = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch} [Val]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(progress_bar):\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, dice_loss, ce_loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice_loss.item()\n",
    "            total_ce += ce_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'dice': f'{dice_loss.item():.4f}',\n",
    "                'ce': f'{ce_loss.item():.4f}'\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_dice = total_dice / len(dataloader)\n",
    "    avg_ce = total_ce / len(dataloader)\n",
    "    \n",
    "    return avg_loss, avg_dice, avg_ce\n",
    "\n",
    "\n",
    "print(\"✓ Training & validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2243e7",
   "metadata": {},
   "source": [
    "SANITY CHECK - RUN 2 BATCHES ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad2ff963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING SANITY CHECK (2 BATCHES)\n",
      "======================================================================\n",
      "\n",
      "Training Pass:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Batch 1/2\n",
      "  Input shape: torch.Size([1, 4, 64, 64, 64])\n",
      "  Mask shape: torch.Size([1, 64, 64, 64])\n",
      "  → Forward pass...\n",
      "  Output shape: torch.Size([1, 4, 64, 64, 64])\n",
      "  → Calculating loss...\n",
      "  ✓ Combined Loss: 1.1421\n",
      "    • Dice Loss: 0.8207\n",
      "    • CE Loss: 1.4634\n",
      "  → Backward pass...\n",
      "  ✓ Gradients computed and weights updated\n",
      "\n",
      "Batch 2/2\n",
      "  Input shape: torch.Size([1, 4, 64, 64, 64])\n",
      "  Mask shape: torch.Size([1, 64, 64, 64])\n",
      "  → Forward pass...\n",
      "  Output shape: torch.Size([1, 4, 64, 64, 64])\n",
      "  → Calculating loss...\n",
      "  ✓ Combined Loss: 1.1580\n",
      "    • Dice Loss: 0.8284\n",
      "    • CE Loss: 1.4876\n",
      "  → Backward pass...\n",
      "  ✓ Gradients computed and weights updated\n",
      "\n",
      "======================================================================\n",
      "SANITY CHECK RESULTS\n",
      "======================================================================\n",
      "Batch 1:\n",
      "  Loss: 1.1421 | Dice: 0.8207 | CE: 1.4634\n",
      "Batch 2:\n",
      "  Loss: 1.1580 | Dice: 0.8284 | CE: 1.4876\n",
      "\n",
      "======================================================================\n",
      "✅ SANITY CHECK PASSED!\n",
      "======================================================================\n",
      "\n",
      "Verifications Complete:\n",
      "  ✓ Model can process input batches\n",
      "  ✓ Output shapes are correct\n",
      "  ✓ Loss functions work properly\n",
      "  ✓ Forward pass successful\n",
      "  ✓ Backward pass successful\n",
      "  ✓ Gradient computation working\n",
      "  ✓ Optimizer updates weights\n",
      "\n",
      "🚀 Ready for Week-4: Full Training!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING SANITY CHECK (2 BATCHES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.train()\n",
    "\n",
    "sanity_results = []\n",
    "\n",
    "print(\"\\nTraining Pass:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "    if batch_idx >= config.SANITY_CHECK_BATCHES:\n",
    "        break\n",
    "    \n",
    "    print(f\"\\nBatch {batch_idx + 1}/{config.SANITY_CHECK_BATCHES}\")\n",
    "    print(f\"  Input shape: {images.shape}\")\n",
    "    print(f\"  Mask shape: {masks.shape}\")\n",
    "    \n",
    "    # Move to device\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    print(\"  → Forward pass...\")\n",
    "    outputs = model(images)\n",
    "    print(f\"  Output shape: {outputs.shape}\")\n",
    "    \n",
    "    # Calculate loss\n",
    "    print(\"  → Calculating loss...\")\n",
    "    loss, dice_loss, ce_loss = criterion(outputs, masks)\n",
    "    \n",
    "    print(f\"  ✓ Combined Loss: {loss.item():.4f}\")\n",
    "    print(f\"    • Dice Loss: {dice_loss.item():.4f}\")\n",
    "    print(f\"    • CE Loss: {ce_loss.item():.4f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    print(\"  → Backward pass...\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"  ✓ Gradients computed and weights updated\")\n",
    "    \n",
    "    sanity_results.append({\n",
    "        'batch': batch_idx + 1,\n",
    "        'loss': loss.item(),\n",
    "        'dice': dice_loss.item(),\n",
    "        'ce': ce_loss.item()\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SANITY CHECK RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for result in sanity_results:\n",
    "    print(f\"Batch {result['batch']}:\")\n",
    "    print(f\"  Loss: {result['loss']:.4f} | Dice: {result['dice']:.4f} | CE: {result['ce']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ SANITY CHECK PASSED!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nVerifications Complete:\")\n",
    "print(\"  ✓ Model can process input batches\")\n",
    "print(\"  ✓ Output shapes are correct\")\n",
    "print(\"  ✓ Loss functions work properly\")\n",
    "print(\"  ✓ Forward pass successful\")\n",
    "print(\"  ✓ Backward pass successful\")\n",
    "print(\"  ✓ Gradient computation working\")\n",
    "print(\"  ✓ Optimizer updates weights\")\n",
    "print(\"\\n🚀 Ready for Week-4: Full Training!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fac22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
